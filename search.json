[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About Me"
  },
  {
    "objectID": "assignment01.html",
    "href": "assignment01.html",
    "title": "Assignment01",
    "section": "",
    "text": "Run Fall.R (on class GitHub under R)\n\n\n\nGive your own colors (e.g. Spring).\nExport the file and post on your GitHub website.\n\n\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\n#install.packages(\"gsubfn\")\n#install.packages(\"tidyverse\")\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %>% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %>% rbind(points)->points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %>%\n      rbind(status) -> status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]->points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %>%\n      rbind(points) -> points\n    status[-1,]->status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"red1\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\n\nWrite a critique on a chart in published work (book/article/news website).\n‘Misleading Statistics’\nScales on Y-axis (no defined) distort the results.To compare the increase/decrease between arguments in a specific period of times, It must same unit and be represented on Y-axis. Then, we can see which has more/less increase/decrease during the time period.\nhttps://www.datapine.com/blog/misleading-statistics-and-data/"
  },
  {
    "objectID": "assignment02.html",
    "href": "assignment02.html",
    "title": "Assignment02",
    "section": "",
    "text": "a. Be sure to run line by line and note the changes\nb. Pay attention to the comments and address the question if there is one\nQuestion 1. plot(pressure, pch=16) # Can you change pch?\nYes, we can change the pch. If we change pch, the shape of the points on the graph changes. For example, if we change pch to 1, points appear as a hollow circle like the first one on the right in the figure below. Also, changing pch to 2, 3, 4 changes the shape of the points on the graph as follows:\nQuestion 2. points(x, y1, pch=16, cex=3) # Try different cex value?\nChanging the cex value changes the size of the point. That is, the larger the number, the larger the size of the point. For example, if you look below, the first picture is when the cex value of the black point is set to 1, and the second picture is when the cex value of the black point is set to 5.\nQuestion 3. axis(1, at=seq(0, 16, 4)) # What is the first number standing for?\nThis code creates an axis. The first number 1 in code “axis(1, at=seq(0, 16, 4))” means the lower horizontal axis among the axes. Thus, running the code “axis(1, at=seq(0, 16, 4))” produces the lower horizontal axis as below:\nc. Plotting functions (note: exercise using the happy planet data set http://happyplanetindex.org)\n\nrm(list=ls())                          # Clear environment\n\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(gridBase)\nlibrary(grid)\n\nHPIdata<-read_excel(\"happy-planet-index2019.xlsx\")\n\n\npar(mfrow=c(3, 2))\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(HPIdata$HPI), c(0, 16))\nlines(HPIdata$HPI, HPIdata$Ladder_of_life_Wellbeing)\nlines(HPIdata$HPI, HPIdata$Ecological_Footprint)\npoints(HPIdata$HPI, HPIdata$Ladder_of_life_Wellbeing, pch=16, cex=2) # Try different cex value?\npoints(HPIdata$HPI, HPIdata$Ecological_Footprint, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(20, 65, 5)) # What is the first number standing for?\naxis(2, at=seq(0, 10, 2))\naxis(4, at=seq(0, 16, 2))\nbox(bty=\"u\")\nmtext(\"HPI\", side=1, line=2, cex=0.8)\nmtext(\"Ladder of life (Wellbeing)\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Ecological_Footprint\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(HPIdata$Ladder_of_life_Wellbeing, breaks=seq(0, 10), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(density(HPIdata$Ladder_of_life_Wellbeing), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\n\nboxplot(Ladder_of_life_Wellbeing ~ Continent, data = HPIdata,\n        boxwex = 0.25, at = 1:8 - 0.2,\n        col=\"white\",\n        xlab=\"\",\n        ylab=\"Ladder_of_life_Wellbeing\", ylim=c(0,10))\n\nmtext(\"Continent\", side=1, line=2.5, cex=0.8)\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Persp\nx <- seq(-10, 10, length= 80)\ny <- x\nf <- function(x,y) { r <- sqrt(x^2+y^2)+7; 10 * tan(r)/r }\nz <- outer(x, y, f)\nz[is.na(z)] <- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 15, phi = 20, \n      expand = 0.5)\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales <- c(0.20, 0.10, 0.30, 0.10, 0.25, 0.05)\nnames(pie.sales) <- c(\"Milk\", \"Water\",\n                      \"Juice\", \"Smoothie\", \"Coffee\", \"Other\")\npie(pie.sales, col = rainbow(6))\n\n# Barplot\n\nHPI1 <- data.frame(HPIdata$Continent, HPIdata$HPI)\n\n\nHPI1 <- transform(HPI1, HPI_grade = ifelse(HPIdata$HPI < 30, \"5\", ifelse(HPIdata$HPI >= 30 & HPIdata$HPI < 40, \"4\", \nifelse(HPIdata$HPI >= 40 & HPIdata$HPI < 50, \"3\", ifelse(HPIdata$HPI >= 50 & HPIdata$HPI < 60, \"2\", \"1\")))))\n\n\n\n\nplot.new()              \nvps <- baseViewports()\npushViewport(vps$figure) \nvp1 <-plotViewport(c(0.1,0.1,0,0))\n\np <- ggplot(HPI1, aes(HPIdata.Continent, fill = HPI_grade)) +\n  geom_bar(position=\"stack\")+ theme(legend.title=element_text(size=10), legend.text=element_text(size=10), legend.key.height= unit(0.1, 'cm'))\nprint(p,vp = vp1)\nprint(p,vp = vp1)"
  },
  {
    "objectID": "assignment03.html",
    "href": "assignment03.html",
    "title": "Assignment03",
    "section": "",
    "text": "a. Compare the regression models\nThe figure shows that the 1st model is relatively suitable (linear relation) compared to the other models. It seems that the 2nd model is not suitable because the coefficient line is a straight line but the plots appear to follow a curvelinear pattern. The 3rd figure shows that the model suggested is not suitable because there is extreme value and this is affecting the coefficient line. Here, the 4th model is not suitable because there are only two x values of the observations, and all x values are at 8 except for one observation.\nb. Compare different ways to create the plots (e.g. changing colors, line types, plot characters)\n\nrm(list=ls())                          # Clear environment\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 <- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 <- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 <- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 <- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff <- y ~ x\nmods <- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] <- as.name(paste0(\"y\", i))\n  ##      ff[[3]] <- as.name(paste0(\"x\", i))\n  mods[[i]] <- lmi <- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"green\", pch = 21, bg = \"blue\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"red\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment03.html#can-you-finetune-the-charts-without-using-other-packages-consult-rgraphics-by-murrell",
    "href": "assignment03.html#can-you-finetune-the-charts-without-using-other-packages-consult-rgraphics-by-murrell",
    "title": "Assignment03",
    "section": "2. Can you finetune the charts without using other packages (consult RGraphics by Murrell)",
    "text": "2. Can you finetune the charts without using other packages (consult RGraphics by Murrell)\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"green\", pch = 20, bg = \"yellow\", cex = 1.5,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"red\")\n}\n\n\n\n\n\n\n\n\n\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 2)"
  },
  {
    "objectID": "assignment03.html#how-about-with-ggplot2-use-tidyverse-package",
    "href": "assignment03.html#how-about-with-ggplot2-use-tidyverse-package",
    "title": "Assignment03",
    "section": "3. How about with ggplot2? (use tidyverse package)",
    "text": "3. How about with ggplot2? (use tidyverse package)\n\nlibrary(ggplot2)\n\n# Figure 1\np1 <- ggplot(anscombe, aes(x1, y1)) +\n  geom_point()\np1 + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Figure 2\np2 <- ggplot(anscombe, aes(x2, y2)) +\n  geom_point()\np2 + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Figure 3\np3 <- ggplot(anscombe, aes(x3, y3)) +\n  geom_point()\np3 + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Figure 4\np4 <- ggplot(anscombe, aes(x4, y4)) +\n  geom_point()\np4 + geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "assignment04.html",
    "href": "assignment04.html",
    "title": "Assignment04",
    "section": "",
    "text": "# Data Visualization\n# Assignment 4-1\n\n# Set Column Names\ndf <- data.frame(x = c(\"Texas\", \"Florida\", \"NewYork\",\n     \"California\"), width = c(25, 50, 75, 100), height = c(100,\n     75, 50, 25))\n\ndf$w <- cumsum(df$width)\ndf$wm <- df$w - df$width\ndf$wt <- with(df, wm + (w - wm)/2)\n\nlibrary(ggplot2)\np <- ggplot(df, aes(ymin = 0))\np1 <- p + geom_rect(aes(xmin = wm, xmax = w,\n     ymax = height, fill = x))\n\np2 <- p1 + geom_text(aes(x = wt, y = height *\n     0.5, label = x))\n\np2 + ggtitle(\"Carbon Emission and Gas Price by State\") + xlab(\"Gas Price\") + ylab(\"Carbon Emission\")\n\n\n\n\nThe figure above shows the relationship between carbon emissions and gas prices by state. California has the highest gas price and the least carbon emission. On the other hand, Texas has the least gas price while the highest carbon emission. The figure illustrates that the higher the gas price, the lower the carbon emissions."
  },
  {
    "objectID": "assignment04.html#table-with-embedded-charts",
    "href": "assignment04.html#table-with-embedded-charts",
    "title": "Assignment04",
    "section": "2. Table with Embedded Charts",
    "text": "2. Table with Embedded Charts\n\n# Data Visualization\n# Assignment 4-2\n\n# Creating Table with Embedded Charts\ncarbon <- read.csv(\"carbon.csv\", header=TRUE)\nhead(carbon)\n\n       State PerCapitaEmission CarbonIntensity Region           Division\n1    Alabama         21.648482        433.6878   West            Pacific\n2     Alaska         46.674987        643.1302  South East South Central\n3    Arizona         12.690212        239.3805  South West South Central\n4   Arkansas         21.539954        500.0933   West           Mountain\n5 California          9.081707        140.3001   West            Pacific\n6   Colorado         15.930418        263.3013   West           Mountain\n\nlibrary(Hmisc)\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::src()       masks Hmisc::src()\n✖ dplyr::summarize() masks Hmisc::summarize()\n\nlibrary(ggplot2)\n\n# Calucating Mean\nmean(carbon$PerCapitaEmission)\n\n[1] 20.29298\n\n\n\n# Creating Dummy Variable\ncarbon$PerCapitaEmission_dummy <- ifelse(carbon$PerCapitaEmission>=20.20489, 1, 0)\n\n# Creating Ordinal Variable\ncarbon$CarbonIntensity_ordinal <- cut2(carbon$CarbonIntensity, m=10)\n\n# \ntable(carbon$PerCapitaEmission_dummy, carbon$CarbonIntensity_ordinal)\n\n   \n    [ 71.2, 197) [196.6, 270) [269.6, 342) [342.4, 500) [500.1,1035]\n  0           11           10           10            3            0\n  1            0            0            0            7           10\n\n\n\ncarbon_df <- data.frame(carbon)\n\n# Code below does not work... \n## p <- ggplot2(carbon_df,aes(CarbonIntensity_ordinal,PerCapitaEmission,fill=as.factor(Division)),size=5)+geom_bar(position=\"dodge\",stat=\"identity\")+facet_wrap(~CarbonIntensity_ordinad,nrow=4)"
  },
  {
    "objectID": "assignment05.html",
    "href": "assignment05.html",
    "title": "Assignment05",
    "section": "",
    "text": "Bar Chart / Column Chart / Circular Area Chart\n\nrm(list=ls())                          # Clear environment\n\n\nlibrary(ggplot2)\nlibrary(fmsb)\n\ndata01 <- data.frame(\n  Year=c(\"1y\", \"1y\", \"2y\", \"2y\", \"3y\", \"3y\", \"4y\", \"4y\", \"5y\", \"5y\"),\n  policy=c(\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"),\n  effect=c(13, 14, 15, 11, 16, 15, 17, 18, 18, 12)\n)\n\nggplot(data01, aes(x = Year, y = effect, fill = policy)) +\n  geom_col(position = position_dodge()) +\n  scale_fill_manual(values = c(\"#f7001d\", \"#0099f7\")) +\n  coord_flip()\n\n\n\nggplot(data01, aes(x = Year, y = effect, fill = policy)) +\n  geom_col(position = position_dodge()) +\n  scale_fill_manual(values = c(\"#f7001d\", \"#0099f7\")) +\n  geom_text(aes(label = effect), position = position_dodge(0.5), vjust = 1, size = 3, color = \"#ffffff\")\n\n\n\ndata02 <- as.data.frame(matrix( c(13, 14, 15, 11, 16, 15, 17, 18, 18, 12) , ncol=5))\ncolnames(data02) <- c(\"1year\", \"2year\", \"3year\", \"4year\", \"5year\")\nrownames(data02) <- c(\"A\", \"B\")\ndata02 <- rbind(rep(20,5) , rep(0,5) , data02)\n\ncolors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )\ncolors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )\n\nradarchart( data02 , axistype=1, \n            title = \"Comparison of the effect between A and B\",\n            pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,\n            cglcol=\"grey\", cglty=1, axislabcol=\"grey\", caxislabels=seq(0,20,5), cglwd=0.8,\n            vlcex=0.8 \n)"
  },
  {
    "objectID": "assignment06.html",
    "href": "assignment06.html",
    "title": "Assignment06",
    "section": "",
    "text": "require(\"readr\")\n\nLoading required package: readr\n\nrequire(\"shiny\")\n\nLoading required package: shiny\n\nlibrary(\"readxl\")\n\n\n\n# Define UI for dataset viewer app ----\nui <- fluidPage(\n  \n  # App title ----\n  titlePanel(\"Datasets\"),\n  \n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n    \n    # Sidebar panel for inputs ----\n    sidebarPanel(\n      \n      # Input: Text for providing a caption ----\n      # Note: Changes made to the caption in the textInput control\n      # are updated in the output area immediately as you type\n      textInput(inputId = \"caption\",\n                label = \"Caption:\",\n                value = \"Datasets\"),\n      \n      # Input: Selector for choosing dataset ----\n      selectInput(inputId = \"dataset\",\n                  label = \"Choose a dataset:\",\n                  choices = c(\"mtcars\", \"USArrests\",\"uspop\", \"pollution\")),\n      \n      # Input: Numeric entry for number of obs to view ----\n      numericInput(inputId = \"obs\",\n                   label = \"Number of observations to view:\",\n                   min=0,\n                   value = 10)\n      \n    ),\n    \n    # Main panel for displaying outputs ----\n    mainPanel(\n      \n      # Output: Formatted text for caption ----\n      h3(textOutput(\"caption\", container = span)),\n      \n      # Output: Verbatim text for data summary ----\n      verbatimTextOutput(\"summary\"),\n      \n      # Output: HTML table with requested number of observations ----\n      tableOutput(\"view\")\n      \n    )\n  )\n)\n\n# Define server logic to summarize and view selected dataset ----\nserver <- function(input, output) {\n  \n  # Return the requested dataset ----\n  # By declaring datasetInput as a reactive expression we ensure\n  # that:\n  #\n  # 1. It is only called when the inputs it depends on changes\n  # 2. The computation and result are shared by all the callers,\n  #    i.e. it only executes a single time\n  datasetInput <- reactive({\n    switch(input$dataset,\n           \"mtcars\" =mtcars,\n           \"USArrests\"=USArrests,\n           \"uspop\"=uspop,\n           \"pollution\"=pollutiondata)\n  })\n  \n  # Create caption ----\n  # The output$caption is computed based on a reactive expression\n  # that returns input$caption. When the user changes the\n  # \"caption\" field:\n  #\n  # 1. This function is automatically called to recompute the output\n  # 2. New caption is pushed back to the browser for re-display\n  #\n  # Note that because the data-oriented reactive expressions\n  # below don't depend on input$caption, those expressions are\n  # NOT called when input$caption changes\n  output$caption <- renderText({\n    input$caption\n  })\n  \n  # Generate a summary of the dataset ----\n  # The output$summary depends on the datasetInput reactive\n  # expression, so will be re-executed whenever datasetInput is\n  # invalidated, i.e. whenever the input$dataset changes\n  output$summary <- renderPrint({\n    dataset <- datasetInput()\n    summary(dataset)\n  })\n  \n  # Show the first \"n\" observations ----\n  # The output$view depends on both the databaseInput reactive\n  # expression and input$obs, so it will be re-executed whenever\n  # input$dataset or input$obs is changed\n  output$view <- renderTable({\n    head(datasetInput(), n = input$obs)\n  })\n  \n}\n\n# Create Shiny app ----\nshinyApp(ui, server)\n\nPhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\n\n\nListening on http://127.0.0.1:4241"
  },
  {
    "objectID": "assignment07.html",
    "href": "assignment07.html",
    "title": "Assignment07",
    "section": "",
    "text": "Screengrab for Shiny App Assignment\n\n#Formatting page \n\n# install.packages(\"shiny\")\nlibrary(\"shiny\")\n\n# First component: User interface (ui)\nui <- fluidPage (\n  tags$h1(\"Data Visualization App\"), \n  tags$img(src = \"\"),\n   # tags$hr(),\n   # tags$br(),\n  tags$h1(strong(\"Youngseok Yoon\")),\n  tags$p(em(\"School of Economic, Political and Policy Sciences\")),\n  tags$a(em(href=\"https://utdallas.edu\", \"University of Texas at Dallas\")),\n  tags$a(href=\"https://youngseoky.github.io/\", \"Youngseok Yoon\"))\n\n# Second component: server\nserver <- function(input , output){} \n\n# Calling the shinyapp\nshinyApp(ui = ui , server = server)\n\nPhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\n\n\nListening on http://127.0.0.1:4902"
  },
  {
    "objectID": "assignment10.html",
    "href": "assignment10.html",
    "title": "Assignment10",
    "section": "",
    "text": "Dr. Patrick Brandt\n\n# CP-Examples.R\n#\n# Patrick T. Brandt\n# pbrandt@utdallas.edu\n#\n# 20221101 : Initial version aggregated from earlier examples\n# 20221106 : Add RW intervention example (with dynamics)\n# 20221114 : Added Jewell et al. 2022 BSS v. L0 example\n#\n#######################################################################\n# Simulated data example\n#######################################################################\n# Do a simulation example first -- breaks in a regression, no dynamics\nset.seed(123)\ny1 <- rnorm(50, mean=0, sd=1)     # Break at 50\ny2 <- rnorm(100, mean=2, sd=0.5)  # Break at 150\ny3 <- rnorm(50, mean=-1, sd=2)\ny <- c(y1,y2,y3)\nplot(ts(y))\n\n# Fit the breakpoint models\nlibrary(strucchange)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\n\n\n# One break\nsystem.time(M1serial <- breakpoints(y ~ 1, h=0.05, breaks=1))\n\n   user  system elapsed \n   0.02    0.00    0.02 \n\n# Two break model\nsystem.time(M2serial <- breakpoints(y ~ 1, h=0.05, breaks=2))\n\n   user  system elapsed \n   0.09    0.00    0.09 \n\n# Three break model\nsystem.time(M3serial <- breakpoints(y ~ 1, h=0.05, breaks=3))\n\n   user  system elapsed \n   0.14    0.00    0.14 \n\n# Summarize the results\nsummary(M1serial)\n\n\n     Optimal (m+1)-segment partition: \n\nCall:\nbreakpoints.formula(formula = y ~ 1, h = 0.05, breaks = 1)\n\nBreakpoints at observation number:\n           \nm = 1   152\n\nCorresponding to breakdates:\n            \nm = 1   0.76\n\nFit:\n               \nm   0     1    \nRSS 549.9 356.8\nBIC 780.5 704.5\n\nsummary(M2serial)\n\n\n     Optimal (m+1)-segment partition: \n\nCall:\nbreakpoints.formula(formula = y ~ 1, h = 0.05, breaks = 2)\n\nBreakpoints at observation number:\n              \nm = 1      152\nm = 2   50 152\n\nCorresponding to breakdates:\n                 \nm = 1        0.76\nm = 2   0.25 0.76\n\nFit:\n                     \nm   0     1     2    \nRSS 549.9 356.8 234.3\nBIC 780.5 704.5 631.0\n\nsummary(M3serial)\n\n\n     Optimal (m+1)-segment partition: \n\nCall:\nbreakpoints.formula(formula = y ~ 1, h = 0.05, breaks = 3)\n\nBreakpoints at observation number:\n                  \nm = 1      152    \nm = 2   50 152    \nm = 3   50 150 174\n\nCorresponding to breakdates:\n                      \nm = 1        0.76     \nm = 2   0.25 0.76     \nm = 3   0.25 0.75 0.87\n\nFit:\n                           \nm   0     1     2     3    \nRSS 549.9 356.8 234.3 226.3\nBIC 780.5 704.5 631.0 634.7"
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "Final Project",
    "section": "",
    "text": "# EPPS 6356 - Data Visualization\n# Pyung Kim et al.\n\nrm(list=ls())\n\n# 1. Data Setting\nload(\"C:/Users/yoond/Documents/EPPS6356F22DataVisualization/youngseoky.github.io/Archive/toxic.rdata\")\nlibrary(sf)    # install.packages(\"sf\") \n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n# 2. Read Seoul grid data (shp file)\ngrid <- st_read(\"C:/Users/yoond/Documents/EPPS6356F22DataVisualization/youngseoky.github.io/Archive/sigun_grid/seoul.shp\")  \n\nReading layer `seoul' from data source \n  `C:\\Users\\yoond\\Documents\\EPPS6356F22DataVisualization\\youngseoky.github.io\\Archive\\sigun_grid\\seoul.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 694 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 126.7645 ymin: 37.42899 xmax: 127.1835 ymax: 37.70146\nGeodetic CRS:  GCS_unknown\n\nemission <-st_join(emission, grid, join = st_intersects)\n\n# 3. Calculate average emission by grid in Seuol, South Korea \nkde_high <- aggregate(emission$py, by=list(emission$ID), mean)\ncolnames(kde_high) <- c(\"ID\", \"avg_emission\")   # Change column names\n\n# 4. Merge GRID and Average Emission\nkde_high <- merge(grid, kde_high,  by=\"ID\")   # Merge by ID\nlibrary(ggplot2) # install.packages(\"ggplot2\")\nlibrary(dplyr)   # install.packages(\"dplyr\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# 5. Emission map by grid in Seoul\n## Without Fill\nkde_high %>% ggplot(aes(fill = )) + \n  geom_sf()\n\n\n\nkde_high %>% ggplot(aes(fill = avg_emission)) + \n  geom_sf() + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n#################################################################\n\n# 6. Change \"sf\" to \"sf\"\n## sf is dataframe. so, it is easy to calculate.\n## sp is shape, which is easy to create maps\n\nlibrary(sp) # install.packages(\"sp\")\nkde_high_sp <- as(st_geometry(kde_high), \"Spatial\")    # sf => sp\n\n\n# 7. Set corners to draw maps\nx <- coordinates(kde_high_sp)[,1]\ny <- coordinates(kde_high_sp)[,2] \n\nl1 <- bbox(kde_high_sp)[1,1] - (bbox(kde_high_sp)[1,1]*0.0001)\nl2 <- bbox(kde_high_sp)[1,2] + (bbox(kde_high_sp)[1,2]*0.0001)\nl3 <- bbox(kde_high_sp)[2,1] - (bbox(kde_high_sp)[2,1]*0.0001)\nl4 <- bbox(kde_high_sp)[2,2] + (bbox(kde_high_sp)[1,1]*0.0001)\n\n# 8. Draw windows\nlibrary(spatstat)  # install.packages(\"spatstat\")\n\nLoading required package: spatstat.data\n\n\nLoading required package: spatstat.geom\n\n\nspatstat.geom 3.0-3\n\n\nLoading required package: spatstat.random\n\n\nspatstat.random 3.0-1\n\n\nLoading required package: spatstat.explore\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nspatstat.explore 3.0-5\n\n\nLoading required package: spatstat.model\n\n\nLoading required package: rpart\n\n\nspatstat.model 3.0-2\n\n\nLoading required package: spatstat.linnet\n\n\nspatstat.linnet 3.0-2\n\n\n\nspatstat 3.0-2 \nFor an introduction to spatstat, type 'beginner' \n\nWindow <- owin(xrange=c(l1,l2), yrange=c(l3,l4)) # Create window\nplot(Window)         # Check Boundaries\n\n\n\nrm(list = c(\"kde_high_sp\", \"emission\", \"l1\", \"l2\", \"l3\", \"l4\")) # Clean variables\n\n# 9. Draw density Graph\np <- ppp(x, y, window=Window) # Creates an object of class \"ppp\" representing a point pattern dataset in the two-dimensional plane.\nDensity <- density.ppp(p, weights=kde_high$avg_emission, # Compute a kernel smoothed intensity function from a point pattern.\n                 sigma = bw.diggle(p), # The smoothing bandwidth\n                 kernel = 'gaussian')  \nplot(Density, main=\"KDE of Emission (with noise)\")\n\n\n\nrm(list = c(\"x\", \"y\", \"Window\",\"p\")) # Clean variables\n\n## In statistics, kernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, \n## i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights. \n## KDE answers a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. \n## In some fields such as signal processing and econometrics it is also termed the Parzen???Rosenblatt window method, \n## after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.\n## One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a naive Bayes classifier,\n## which can improve its prediction accuracy.\n\n\n\n# 10. Raster Map (below explanation on KDE came from Wikipedia)\n\nDensity[Density < quantile(Density)[4] + (quantile(Density)[4]*0.1)] <- NA   # Erase Noise\nlibrary(raster)      #  install.packages(\"raster\")\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:nlme':\n\n    getData\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(rworldmap)\n\n### Welcome to rworldmap ###\n\n\nFor a short introduction type :      vignette('rworldmap')\n\nraster_high <- raster(Density)  # Change by raster\nplot(raster_high)\n\n\n\nplot(raster_high, col = colorRamps::matlab.like(n=12),  main=\"KDE of Emission (w/o noise)\")\n\n\n\n# 11. Map grooming\n## Load Seoul shp file\nbnd <- st_read(\"C:/Users/yoond/Documents/EPPS6356F22DataVisualization/youngseoky.github.io/Archive/sigun_bnd/seoul.shp\")\n\nReading layer `seoul' from data source \n  `C:\\Users\\yoond\\Documents\\EPPS6356F22DataVisualization\\youngseoky.github.io\\Archive\\sigun_bnd\\seoul.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 126.7645 ymin: 37.42899 xmax: 127.1835 ymax: 37.70146\nGeodetic CRS:  GCS_unknown\n\nraster_high <- crop(raster_high, extent(bnd))      # Crop unnecessaries\ncrs(raster_high) <- sp::CRS(\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 + towgs84=0,0,0\") # Define coordinates\nplot(raster_high, col = colorRamps::matlab.like(n=12), main=\"KDE of Emission (w/o noise)\")\nplot(bnd, col=NA, border = \"green\", add=TRUE)\n\n\n\n## 12. Put raster on the map\nlibrary(rgdal)    # install.packages(\"rgdal\")\n\nPlease note that rgdal will be retired during 2023,\nplan transition to sf/stars/terra functions using GDAL and PROJ\nat your earliest convenience.\nSee https://r-spatial.org/r/2022/04/12/evolution.html and https://github.com/r-spatial/evolution\nrgdal: version: 1.6-2, (SVN revision 1183)\nGeospatial Data Abstraction Library extensions to R successfully loaded\nLoaded GDAL runtime: GDAL 3.5.2, released 2022/09/02\nPath to GDAL shared files: C:/Users/yoond/AppData/Local/R/win-library/4.2/rgdal/gdal\nGDAL binary built with GEOS: TRUE \nLoaded PROJ runtime: Rel. 8.2.1, January 1st, 2022, [PJ_VERSION: 821]\nPath to PROJ shared files: C:/Users/yoond/AppData/Local/R/win-library/4.2/rgdal/proj\nPROJ CDN enabled: FALSE\nLinking to sp version:1.5-1\nTo mute warnings of possible GDAL/OSR exportToProj4() degradation,\nuse options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n\nlibrary(leaflet)  # install.packages(\"leaflet\")\nleaflet() %>% # Creates a Leaflet map widget using htmlwidget\n  #Base Map\n  addProviderTiles(providers$CartoDB.Positron) %>% \n  #Call Boundaries\n  addPolygons(data = bnd, weight = 3, color= \"green\", fill = NA) %>% \n  #Add Raster Map\n  addRasterImage(raster_high, \n                 colors = colorNumeric(c(\"green\", \"yellow\",\"orange\",\"red\"), \n                                       values(raster_high), na.color = \"transparent\"), opacity = 0.4)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome!\nI am a doctoral student studying public administration at University of Texas at Dallas.\n\n\n\n\n\n\n\n\n\nResearch Interests\n\nPublic Management\nExperimental Research\nData Analytics\n\n\n\n\n\n\n\n\n\n\nContact\nyoungseok.yoon@utdallas.edu\nhttps://youngseoky.github.io"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My Research"
  }
]